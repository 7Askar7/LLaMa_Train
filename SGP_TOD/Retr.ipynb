{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e567f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from sentence-transformers) (4.30.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from sentence-transformers) (1.9.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.3)\n",
      "Requirement already satisfied: click in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (2.14.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: openai in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (0.27.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\askar\\anaconda3\\envs\\env310\\lib\\site-packages (from tqdm->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install datasets\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80274a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Askar\\anaconda3\\envs\\env310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import logging\n",
    "import faiss\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Callable, Any\n",
    "import psutil\n",
    "\n",
    "class ScalableSemanticSearch:\n",
    "    \"\"\"Vector similarity using product quantization with sentence transformers embeddings and cosine similarity.\"\"\"\n",
    "\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.model = SentenceTransformer(\n",
    "            \"sentence-transformers/all-mpnet-base-v2\", device=self.device\n",
    "        )\n",
    "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        self.quantizer = None\n",
    "        self.index = None\n",
    "        self.hashmap_index_sentence = None\n",
    "\n",
    "        log_directory = \"log\"\n",
    "        if not os.path.exists(log_directory):\n",
    "            os.makedirs(log_directory)\n",
    "        log_file_path = os.path.join(log_directory, \"scalable_semantic_search.log\")\n",
    "\n",
    "        logging.basicConfig(\n",
    "            filename=log_file_path,\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "        )\n",
    "        logging.info(\"ScalableSemanticSearch initialized with device: %s\", self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_clusters(n_data_points: int) -> int:\n",
    "        return max(2, min(n_data_points, int(np.sqrt(n_data_points))))\n",
    "\n",
    "    def encode(self, data: List[str]) -> np.ndarray:\n",
    "        \"\"\"Encode input data using sentence transformer model.\n",
    "\n",
    "        Args:\n",
    "            data: List of input sentences.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array of encoded sentences.\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(data)\n",
    "        self.hashmap_index_sentence = self.index_to_sentence_map(data)\n",
    "        return embeddings.astype(\"float32\")\n",
    "\n",
    "    def build_index(self, embeddings: np.ndarray) -> None:\n",
    "        \"\"\"Build the index for FAISS search.\n",
    "\n",
    "        Args:\n",
    "            embeddings: Numpy array of encoded sentences.\n",
    "        \"\"\"\n",
    "        n_data_points = len(embeddings)\n",
    "        if (\n",
    "            n_data_points >= 1500\n",
    "        ):  # Adjust this value based on the minimum number of data points required for IndexIVFPQ\n",
    "            self.quantizer = faiss.IndexFlatL2(self.dimension)\n",
    "            n_clusters = self.calculate_clusters(n_data_points)\n",
    "            self.index = faiss.IndexIVFPQ(\n",
    "                self.quantizer, self.dimension, n_clusters, 8, 4\n",
    "            )\n",
    "            logging.info(\"IndexIVFPQ created with %d clusters\", n_clusters)\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatL2(self.dimension)\n",
    "            logging.info(\"IndexFlatL2 created\")\n",
    "\n",
    "        if isinstance(self.index, faiss.IndexIVFPQ):\n",
    "            self.index.train(embeddings)\n",
    "        self.index.add(embeddings)\n",
    "        logging.info(\"Index built on device: %s\", self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def index_to_sentence_map(data: List[str]) -> Dict[int, str]:\n",
    "        \"\"\"Create a mapping between index and sentence.\n",
    "\n",
    "        Args:\n",
    "            data: List of sentences.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping index to the corresponding sentence.\n",
    "        \"\"\"\n",
    "        return {index: sentence for index, sentence in enumerate(data)}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_top_sentences(\n",
    "        index_map: Dict[int, str], top_indices: np.ndarray\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Get the top sentences based on the indices.\n",
    "\n",
    "        Args:\n",
    "            index_map: Dictionary mapping index to the corresponding sentence.\n",
    "            top_indices: Numpy array of top indices.\n",
    "\n",
    "        Returns:\n",
    "            List of top sentences.\n",
    "        \"\"\"\n",
    "        return [index_map[i] for i in top_indices]\n",
    "\n",
    "    def search(self, input_sentence: str, top: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Compute cosine similarity between an input sentence and a collection of sentence embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_sentence: The input sentence to compute similarity against.\n",
    "            top: The number of results to return.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing two numpy arrays. The first array contains the cosine similarities between the input\n",
    "            sentence and the embeddings, ordered in descending order. The second array contains the indices of the\n",
    "            corresponding embeddings in the original array, also ordered by descending similarity.\n",
    "        \"\"\"\n",
    "        vectorized_input = self.model.encode(\n",
    "            [input_sentence], device=self.device\n",
    "        ).astype(\"float32\")\n",
    "        D, I = self.index.search(vectorized_input, top)\n",
    "        return I[0], 1 - D[0]\n",
    "\n",
    "    def save_index(self, file_path: str) -> None:\n",
    "        \"\"\"Save the FAISS index to disk.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path where the index will be saved.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"index\"):\n",
    "            faiss.write_index(self.index, file_path)\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"The index has not been built yet. Build the index using `build_index` method first.\"\n",
    "            )\n",
    "\n",
    "    def load_index(self, file_path: str) -> None:\n",
    "        \"\"\"Load a previously saved FAISS index from disk.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path where the index is stored.\n",
    "        \"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            self.index = faiss.read_index(file_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"The specified file '{file_path}' does not exist.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def measure_time(func: Callable, *args, **kwargs) -> Tuple[float, Any]:\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        return elapsed_time, result\n",
    "\n",
    "    @staticmethod\n",
    "    def measure_memory_usage() -> float:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        ram = process.memory_info().rss\n",
    "        return ram / (1024**2)\n",
    "\n",
    "    def timed_train(self, data: List[str]) -> Tuple[float, float]:\n",
    "        start_time = time.time()\n",
    "        embeddings = self.encode(data)\n",
    "        self.build_index(embeddings)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        memory_usage = self.measure_memory_usage()\n",
    "        logging.info(\n",
    "            \"Training time: %.2f seconds on device: %s\", elapsed_time, self.device\n",
    "        )\n",
    "        logging.info(\"Training memory usage: %.2f MB\", memory_usage)\n",
    "        return elapsed_time, memory_usage\n",
    "\n",
    "    def timed_infer(self, query: str, top: int) -> Tuple[float, float]:\n",
    "        start_time = time.time()\n",
    "        _, _ = self.search(query, top)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        memory_usage = self.measure_memory_usage()\n",
    "        logging.info(\n",
    "            \"Inference time: %.2f seconds on device: %s\", elapsed_time, self.device\n",
    "        )\n",
    "        logging.info(\"Inference memory usage: %.2f MB\", memory_usage)\n",
    "        return elapsed_time, memory_usage\n",
    "\n",
    "    def timed_load_index(self, file_path: str) -> float:\n",
    "        start_time = time.time()\n",
    "        self.load_index(file_path)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        logging.info(\n",
    "            \"Index loading time: %.2f seconds on device: %s\", elapsed_time, self.device\n",
    "        )\n",
    "        return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc5c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Callable, Any\n",
    "\n",
    "\n",
    "class DST_Prompter:\n",
    "\n",
    "    def __init__(self, query: str):\n",
    "        self.preliminary_data = []\n",
    "        self.query = query\n",
    "        self.data = self._data_load()\n",
    "\n",
    "    def _data_load(self):\n",
    "        \"\"\"\n",
    "        Load data and convert to dict.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(r\"C:\\\\Users\\\\Askar\\\\VS_CODE\\\\Python\\\\PROJECT\\\\AIRPLANE\\\\Data.csv\")\n",
    "        self.data = self.data.to_dict('records')\n",
    "        return self.data\n",
    "\n",
    "    def _get_all_text(self):\n",
    "        all_text = []\n",
    "        for text in self.data:\n",
    "            all_text.append(text[\"Text\"])\n",
    "        return all_text\n",
    "    \n",
    "    def Belief_instructions(self):\n",
    "        belief_list = []\n",
    "\n",
    "\n",
    "        keys = []\n",
    "        for part in self.data:\n",
    "            for key in part.keys():\n",
    "                keys.append(key)\n",
    "\n",
    "        keys = list(set(keys))\n",
    "        keys.remove(\"Text\")\n",
    "\n",
    "        for key in keys:\n",
    "            dict_of_part = {}\n",
    "            val_key = []\n",
    "            \n",
    "            for d in self.data:\n",
    "\n",
    "                if pd.isna(d[key]):\n",
    "                    continue\n",
    "                else:\n",
    "\n",
    "                    if len(val_key) <= 15:\n",
    "                        val_key.append(d[key])\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "            dict_of_part[key] = val_key\n",
    "            belief_list.append(dict_of_part)\n",
    "            \n",
    "        return belief_list\n",
    "\n",
    "    def _semantic_search(self):\n",
    "\n",
    "        semantic_search = ScalableSemanticSearch(device=\"cpu\")\n",
    "        corpus = self._get_all_text()\n",
    "        embeddings = semantic_search.encode(corpus)\n",
    "        semantic_search.build_index(embeddings)\n",
    "        top_indices, top_scores = semantic_search.search(self.query, top=5)\n",
    "        top_sentences = ScalableSemanticSearch.get_top_sentences(semantic_search.hashmap_index_sentence, top_indices)\n",
    "        return top_sentences[0]\n",
    "\n",
    "    def formatting_example(self):\n",
    "        \n",
    "        sentence = self._semantic_search()\n",
    "\n",
    "        all_text = self._get_all_text()  # Corrected line\n",
    "        if sentence in all_text:\n",
    "            index = all_text.index(sentence)\n",
    "        \n",
    "        return self.data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bf98b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = DST_Prompter(\"Здравствуйте! Меня зовут Фичиков Олег Сергеевич. Хочу забронировать один билет с багажом из Сочи в Москву 20 июля в 10:00.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c3c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:16<00:00,  5.39s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.91it/s]\n"
     ]
    }
   ],
   "source": [
    "close_sentence = dst.formatting_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "zapros = \"Здравствуйте! Меня зовут Валуев Олег Сергеевич. Хочу забронировать один билет с багажом из Новосибирска в Москву 20 июля в 10:00.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f95e0652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Имя\": [\"Валуев\"],\n",
      "    \"Фамилия\": [\"Олег\"],\n",
      "    \"Отчество\": [\"Сергеевич\"],\n",
      "    \"Откуда\": [\"Новосибирск\"],\n",
      "    \"Куда\": [\"Москва\"],\n",
      "    \"Дата вылета\": [\"20.07.2023\"],\n",
      "    \"Время вылета\": [\"10:00\"],\n",
      "    \"Количество взрослых\": [1],\n",
      "    \"Количество детей\": [0.0],\n",
      "    \"Класс\": [\"Бизнес\"],\n",
      "    \"Багажник\": [\"Да\"]\n",
      "}"
     ]
    }
   ],
   "source": [
    "# pip install llama-cpp-python fire\n",
    "import requests\n",
    "import fire\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "SYSTEM_TOKEN = 1788\n",
    "USER_TOKEN = 1404\n",
    "BOT_TOKEN = 9225\n",
    "LINEBREAK_TOKEN = 13\n",
    "\n",
    "ROLE_TOKENS = {\n",
    "    \"user\": USER_TOKEN,\n",
    "    \"bot\": BOT_TOKEN,\n",
    "    \"system\": SYSTEM_TOKEN\n",
    "}\n",
    "\n",
    "\n",
    "def get_message_tokens(model, role, content):\n",
    "    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n",
    "    message_tokens.insert(1, ROLE_TOKENS[role])\n",
    "    message_tokens.insert(2, LINEBREAK_TOKEN)\n",
    "    message_tokens.append(model.token_eos())\n",
    "    return message_tokens\n",
    "\n",
    "\n",
    "def get_system_tokens(model):\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": SYSTEM_PROMPT\n",
    "    }\n",
    "    return get_message_tokens(model, **system_message)\n",
    "\n",
    "model_path = r\"C:\\\\Users\\\\Askar\\\\Desktop\\\\nlp_api\\\\model-q8_0.gguf\"\n",
    "#model_path = r\"C:\\\\Users\\\\Askar\\\\VS_CODE\\\\Python\\\\WORK\\\\ChatBot\\\\LLama2\\\\saiga_checkpoints\\\\ggml-model-q3_K.gguf\"\n",
    "n_ctx = 2000\n",
    "top_k = 30\n",
    "top_p = 0.9\n",
    "temperature = 0.2\n",
    "repeat_penalty = 1.1\n",
    "\n",
    "model = Llama(model_path=model_path, n_ctx=n_ctx, n_parts=1)\n",
    "\n",
    "system_tokens = get_system_tokens(model)\n",
    "tokens = system_tokens\n",
    "model.eval(tokens)\n",
    "\n",
    "\n",
    "user_message = f\"\"\"User: Следуя ПРИМЕРАМ, Ивзлеки эти данные из предложения: Фамилия, Имя, Отчество, Откуда, Куда, Дата вылета, Время вылета, Количество взрослых,Количество детей,Класс,Багаж.\n",
    "                    Эти ПРМИЕРЫ поможгут тебе получить необходимые данные из запроса. Не используйте эти ПРИМЕРЫ самом запросе. ПРИМЕРЫ :{dst.Belief_instructions()}, ПРИМЕР: {close_sentence}.\n",
    "                    Предложение = '{zapros}'. Напиши только полученные данные! \n",
    "                     \"\"\"\n",
    "message_tokens = get_message_tokens(model=model, role=\"user\", content=user_message)\n",
    "role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n",
    "tokens += message_tokens + role_tokens\n",
    "generator = model.generate(\n",
    "    tokens,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    temp=temperature,\n",
    "    repeat_penalty=repeat_penalty\n",
    ")\n",
    "for token in generator:\n",
    "    token_str = model.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\n",
    "    tokens.append(token)\n",
    "    if token == model.token_eos():\n",
    "        break\n",
    "    print(token_str, end=\"\", flush=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C:\\\\Users\\\\Askar\\\\Desktop\\\\nlp_api\\\\model-q8_0.gguf\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
